import json
import os
# from multiprocessing import Pool, cpu_count
from itertools import combinations
from tqdm import tqdm
from collections import defaultdict
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords




'''å¯ä¿®æ”¹åƒæ•¸'''
FOLDER_PATH = "../temp_data"  # é¸æ“‡è¦å°å“ªå€‹è³‡æ–™å¤¾åŸ·è¡Œ
# "../Kmeans/data/clustered/"
# "../project_vscode/data/spammer/04"

OUTPUT_FOLDER_NAME = "20210416"  # è¨­å®šè¦å„²å­˜åˆ°çš„è³‡æ–™å¤¾åç¨±   ex. "../LCS/analysis/{OUTPUT_FOLDER_NAME}/"

JSON_DICT_NAME = "dogecoin"  # è¨­å®šæ¨æ–‡æ‰€å­˜çš„ json æª”ä¸­å­—å…¸çš„åç¨±

DICE_COEFFICIENT = 70  # è¨­å®š Dice ç®—å‡ºä¾†çš„çµæœé–€æª»å€¼ï¼ˆä¹Ÿå°±æ˜¯ç›¸ä¼¼åº¦ï¼‰  60 => 60%

LENGTH_RATIO = 80  # è¨­å®š Y(è¢«æ¯”å°çš„æ¨æ–‡) çš„é•·åº¦ç›¸å°æ–¼ X(ç•¶åŸºæº–çš„æ¨æ–‡) çš„ç™¾åˆ†æ¯”   80 => 80%
# é€™æ˜¯è¦ç¢ºèªå…©ç¯‡æ¨æ–‡çš„é•·åº¦è½åœ¨åˆç†ç¯„åœå…§ï¼Œæ¨æ–‡ Y çš„é•·åº¦è‡³å°‘æœ‰ X çš„ 80% é•·ï¼ˆä¸èƒ½å¤ªçŸ­ï¼‰

IS_CLUSTERED = False  # è¨­å®šæ˜¯å¦è¦ç”¨æœ‰åˆ†ç¾¤çš„æª”æ¡ˆä¾†æ¯”å°

# ç¬¬ä¸€æ¬¡ä½¿ç”¨æ™‚ä¸‹è¼‰
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('punkt_tab')
'''å¯ä¿®æ”¹åƒæ•¸'''






# å–å¾—è‹±æ–‡åœç”¨è©é›†åˆ
stop_words = set(stopwords.words('english'))

# è¨ˆç®— Dice
def dice(X, Y):
    # å°‡ X Y ç”¨ NLTK çš„ word_tokenize åšå–®å­—åˆ‡å‰²
    # åˆ©ç”¨ lower() æŠŠæ‰€æœ‰å­—æ¯éƒ½è®Šå°å¯«
    x_tokens = word_tokenize(X.lower(), language='english')
    y_tokens = word_tokenize(Y.lower(), language='english')

    # ç§»é™¤ stopwords
    # åˆ©ç”¨ word.isalnum() éæ¿¾æ‰æ¨™é»ç¬¦è™Ÿ   ex. "!".isalnum() â†’ Falseï¼ˆæœ‰é©šå˜†è™Ÿï¼‰   "dogecoin".isalnum() â†’ True
    x_tokens = [word for word in x_tokens if word.isalnum() and (word not in stop_words)]  # é€™è£¡æ˜¯æŠŠæ¯å€‹ token åˆ†åˆ¥æ‹¿å‡ºä¾†æª¢æŸ¥
    y_tokens = [word for word in y_tokens if word.isalnum() and (word not in stop_words)]

    # è½‰æˆé›†åˆä»¥ä¾¿å¾ŒçºŒæ¯”å°
    # ä½†å¦‚æœæœ‰é‡è¤‡çš„å–®å­— æœƒåªç®—æˆä¸€æ¬¡
    x_set = set(x_tokens)
    y_set = set(y_tokens)

    # åœ¨ set çš„æ ¼å¼ä¸­ & ç‚ºè¯é›†
    overlap = len(x_set & y_set)
    total = len(x_set) + len(y_set)

    if total == 0:
        return 0.0, x_tokens, y_tokens  # é¿å…é™¤ä»¥é›¶

    # è¨ˆç®— Dice Coefficient = (2 * |X & Y|) / (|X| + |Y|)
    dice_score = (2 * overlap) / total
    return dice_score, x_tokens, y_tokens

    


def compare_pair(args):
    i, j, tweets = args
    X = tweets[i]
    Y = tweets[j]

    # å¦‚æœæ¯”å°çš„æ¨æ–‡é•·åº¦ç›¸å·®è¶…é LENGTH_RATIO(%) å‰‡ç›´æ¥ä¸åŸ·è¡Œ LCS
    length_ratio_X = (len(Y["text"]) / len(X["text"]))
    length_ratio_Y = (len(X["text"]) / len(Y["text"]))
    if length_ratio_X * 100 < LENGTH_RATIO or length_ratio_Y * 100 < LENGTH_RATIO:
        return None

    # å›å‚³ Dice Coefficient
    dice_coefficient, x_tokens, y_tokens = dice(X["text"], Y["text"])
    # åˆ¤æ–·ç›¸ä¼¼åº¦é«˜æ–¼ DICE_COEFFICIENT(%) æ‰è¼¸å‡ºåˆ° txt æª”è£¡
    if dice_coefficient * 100 < DICE_COEFFICIENT:
        return None

    return {
        "X": X,
        "Y": Y,
        "X_token": ", ".join(x_tokens),
        "Y_token": ", ".join(y_tokens),
        "length_ratio_X": length_ratio_X,
        "length_ratio_Y": length_ratio_Y,
        "dice_coefficient": dice_coefficient
    }



# ç‚ºäº†ä¸è¦ MemoryError
# é‚Šç”¨ imap_unordered é‚Šç”¢ç”Ÿçµ„åˆï¼Œä¸éœ€è¦ä¸€æ¬¡ç”Ÿæˆæ‰€æœ‰çµ„åˆ
def generate_pairs(tweets):
    # (1) é¿å…èˆ‡è‡ªå·±æ¯”å°  (2) æ¯ä¸€çµ„åªæœƒæ¯”å°ä¸€æ¬¡
    # ç”Ÿæˆè¦æ¯”å°çš„æ¨æ–‡çµ„åˆï¼Œæ ¼å¼æ˜¯ (i, j, tweets)
    for i, j in combinations(range(len(tweets)), 2):
        yield (i, j, tweets)

    # return ((i, j, tweets) for i, j in combinations(range(len(tweets)), 2))


def write_txt_result(filetxt, res):
    X, Y = res["X"], res["Y"]

    filetxt.write(f"X = [{repr(X['text'])[1:-1]}]\n")  # repr(): è®“ \n ä¿æŒç‚º \n è¼¸å‡º
    filetxt.write(f"X_token = [{res['X_token']}]\n")
    filetxt.write(f"\tX tweet_count = [{X['tweet_count']}]\n")
    filetxt.write(f"\tX username = [{X['username']}]\n")

    filetxt.write(f"Y = [{repr(Y['text'])[1:-1]}]\n")
    filetxt.write(f"Y_token = [{res['Y_token']}]\n")
    filetxt.write(f"\tY tweet_count = [{Y['tweet_count']}]\n")
    filetxt.write(f"\tY username = [{Y['username']}]\n")

    filetxt.write(f"Total Length: X = {len(X['text'])}, Y = {len(Y['text'])} "
                  f"(Y / X = {res['length_ratio_X'] * 100:.2f}  X / Y = {res['length_ratio_Y'] * 100:.2f})\n")
    filetxt.write(f"Dice Coefficient: {res['dice_coefficient'] * 100:.2f}% \n\n")



def write_json_result(res, cluster_id=None):
    X, Y = res["X"], res["Y"]
    data = {
        "X_text": X["text"],
        "X_token": res["X_token"],
        "X_tweet_count": X["tweet_count"],
        "X_username": X["username"],
        "Y_text": Y["text"],
        "Y_token": res["Y_token"],
        "Y_tweet_count": Y["tweet_count"],
        "Y_username": Y["username"],
        "X_length": len(X["text"]),
        "Y_length": len(Y["text"]),
        "Y_length_percent_of_X": round(res['length_ratio_X'] * 100, 2),
        "X_length_percent_of_Y": round(res['length_ratio_Y'] * 100, 2),
        "dice_coefficient": round(res['dice_coefficient'] * 100, 2)  # round( , 2) è¼¸å‡ºåˆ°å°æ•¸é»å¾Œç¬¬äºŒä½
    }
    if cluster_id is not None:
        data["cluster_id"] = cluster_id
    return data



def process_tweet_group(tweets_group, json_output, json_output_path, cluster_id=None, filetxt=None):
    writed_compare = 0  # å¯¦éš›å¯«å…¥çš„çµæœæ•¸
    fail_count = 0

    os.makedirs(os.path.dirname(json_output_path), exist_ok=True)

    # map(compare_pair, pairs): å°‡ pairs ä¸­çš„æ¯çµ„ (i, j, tweets) å‚³é€² compare_pair å‡½å¼è™•ç†ï¼Œä¸¦è¡Œè™•ç†ï¼Œèª°å…ˆå®Œæˆå°±å…ˆé€å›ä¾†
    for res in tqdm(map(compare_pair, generate_pairs(tweets_group)),  # tqdm(...): é€²åº¦æ¢
                    total=(len(tweets_group) * (len(tweets_group) - 1)) // 2,  # ç”¢ç”Ÿæ‰€æœ‰å¾ n å€‹å…ƒç´ ä¸­é¸å‡º 2 å€‹ä¸é‡è¤‡ä¸”ç„¡é †åºçš„çµ„åˆ  [C(n, 2) = (n(n - 1)) / 2]
                    desc=f"æ¯”å°ä¸­ (Cluster {cluster_id})" if cluster_id is not None else "æ¯”å°ä¸­"):
        
        if res is None:  # å¦‚æœæ˜¯ Noneï¼Œä»£è¡¨æ¯”å°ä¸é€šéè¢«éæ¿¾æ‰
            continue  # ä¸å¯«é€² txt, json æª”

        writed_compare += 1
        json_output.append(write_json_result(res, cluster_id=cluster_id))

        # å¯«å…¥ txt
        write_txt_result(filetxt, res)


    # å…¨éƒ¨æ¯”å°å®Œå†å¯«å…¥ JSON
    try:
        with open(json_output_path, "w", encoding="utf-8") as f:
            json.dump(json_output, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"[ERROR] Failed to write after fail_count = {fail_count}: {e}")
        fail_count += 1
    

    return writed_compare



# ğŸ§  ä¸»ç¨‹å¼å…¥å£ï¼šè™•ç†æ•´å€‹è³‡æ–™å¤¾
if __name__ == "__main__":
    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(".json")]
    print(f"ğŸ“‚ ç¸½å…±æ‰¾åˆ° {len(all_files)} å€‹æª”æ¡ˆè¦è™•ç†")

    # å…ˆæ¸…ç©º robottxt.txt
    robottxt = f"./robot_account/{OUTPUT_FOLDER_NAME}.txt"
    with open(robottxt, "w", encoding="utf-8-sig") as robotfile:
        robotfile.write("")

    for file in all_files:
        # run_for_file(os.path.join(FOLDER_PATH, file))
        filepath = os.path.join(FOLDER_PATH, file)
        filename = os.path.basename(filepath)  # ex: DOGE_20210428.json
        analysis_name = os.path.splitext(filename)[0]  # ex: DOGE_20210428

        # è¨­å®š txtname, json_output_path çš„åç¨±
        txtname = f"./analysis/{OUTPUT_FOLDER_NAME}/{analysis_name}.txt"
        json_output_path = f"./analysis/{OUTPUT_FOLDER_NAME}/{analysis_name}.json"

        # ç¢ºèªæ˜¯å¦æœ‰è¼¸å‡ºæ™‚éœ€ä½¿ç”¨çš„è³‡æ–™å¤¾
        output_folder_path = f"./analysis/{OUTPUT_FOLDER_NAME}/"
        os.makedirs(output_folder_path, exist_ok=True)

        # è®€å…¥ json æª”
        with open(filepath, 'r', encoding="utf-8-sig") as file:
            data_json = json.load(file)

        tweets = data_json[JSON_DICT_NAME]
        print(f"\nğŸ“„ æ­£åœ¨è™•ç†æª”æ¡ˆï¼š{filename}ï¼Œå…± {len(tweets)} ç­†æ¨æ–‡")

        # å…ˆæŠŠ txt æª”è£¡æ¸…ç©º
        with open(txtname, 'w', encoding="utf-8-sig") as filetxt:
            filetxt.write("")

        json_output = []  # ç”¨ä¾†å„²å­˜æ‰€æœ‰æ¯”å°çµæœ

        # å…ˆæŠŠè¼¸å‡ºçš„ json æª”è£¡æ¸…ç©º
        with open(json_output_path, 'w', encoding='utf-8-sig') as f_json:
            json.dump(json_output, f_json, indent=4, ensure_ascii=False)



        total_compare = 0  # è¨ˆç®—ç¸½å…±å¯«å…¥çš„çµæœæ•¸

        with open(txtname, 'w', encoding="utf-8-sig") as filetxt:
            if IS_CLUSTERED:
                # defaultdict: Python çš„ä¸€ç¨®ç‰¹æ®Šå­—å…¸
                # ç•¶ä½ å­˜å–ä¸€å€‹ä¸å­˜åœ¨çš„ key æ™‚ï¼Œå®ƒæœƒè‡ªå‹•å»ºç«‹å°æ‡‰çš„é è¨­å€¼  ex. clusters["0"] è‹¥åŸæœ¬ä¸å­˜åœ¨ï¼Œæœƒè‡ªå‹•è¢«å»ºç«‹æˆ []ï¼ˆç©º listï¼‰
                clusters = defaultdict(list)

                for tweet in tweets:
                    clusters[tweet["cluster"]].append(tweet)  # æŠŠæ¨æ–‡åŠ å…¥å°æ‡‰çš„ç¾¤é›†  ex. clusters[0].append(æ¨æ–‡)

                # .items() ä¾†ä¸€æ¬¡å–å¾— keyï¼ˆcluster_idï¼‰èˆ‡å°æ‡‰çš„ valueï¼ˆcluster_tweetsï¼Œä¸€å€‹ listï¼‰
                for cluster_id, cluster_tweets in clusters.items():
                    if len(cluster_tweets) < 2:
                        continue  # ä¸éœ€è¦æ¯”å°

                    filetxt.write(f"cluster {cluster_id}, å…± {len(cluster_tweets)} ç­†\n")

                    # å‘¼å« process_tweet_group ä¾†åŸ·è¡Œæ¯”å°ï¼Œä¸¦å›å‚³ç•¶å‰ Cluster çš„å¯¦éš›å¯«å…¥æ•¸é‡
                    total_compare += process_tweet_group(cluster_tweets, json_output, json_output_path, cluster_id=cluster_id, filetxt=filetxt)
            else:
                # å¦‚æœæ˜¯æ²’æœ‰åˆ†é¡éçš„æª”æ¡ˆ ç›´æ¥å‘¼å« process_tweet_group ä¾†åŸ·è¡Œæ¯”å°
                total_compare = process_tweet_group(tweets, json_output, json_output_path, filetxt=filetxt)

        print()
        print(f"âœ… å·²å„²å­˜ JSON çµæœåˆ° {json_output_path}")
        print(f"å¯¦éš›å¯«å…¥çš„å…¨éƒ¨çµæœæ•¸ï¼š{total_compare}")
        print(f"âœ… å·²è¼¸å‡ºçµæœåˆ° {txtname}")



        # å»ºç«‹ä¸€å€‹å­—å…¸è¨˜éŒ„æ¯å€‹å¸³è™Ÿæœ‰å¤šå°‘é‡è¤‡æ¨æ–‡
        repetitive_counts = defaultdict(int)

        # ç•¶ä½ å¾ Dice å°æ¯”çµæœä¸­æŠ“å‡ºé‡è¤‡æ¨æ–‡æ™‚
        # ä½ å¯ä»¥è¨˜éŒ„å¸³è™Ÿå‡ºç¾çš„æ¬¡æ•¸
        for tweet in json_output:
            X_user = tweet["X_username"]
            Y_user = tweet["Y_username"]
            repetitive_counts[X_user] += 1
            repetitive_counts[Y_user] += 1


        robottxt = f"./robot_account/{OUTPUT_FOLDER_NAME}.txt"
        # å°å‡ºå‡ºç¾æ¬¡æ•¸å¤§æ–¼ 10 çš„å¸³è™Ÿï¼Œç¬¦åˆçš„è©±å°±è¼¸å‡ºåˆ° txt æª”ä¸­
        print()
        with open(robottxt, "a", encoding="utf-8-sig") as robotfile:
            robotfile.write(f"{filename}\n")
            for user, count in sorted(repetitive_counts.items(), key=lambda x: x[1], reverse=True):
                if count > 10:
                    robotfile.write(f"ğŸ¤– ç–‘ä¼¼æ´—ç‰ˆå¸³è™Ÿï¼š{user}ï¼Œé‡è¤‡å‡ºç¾æ¬¡æ•¸ï¼š{count}\n")
            robotfile.write("\n")