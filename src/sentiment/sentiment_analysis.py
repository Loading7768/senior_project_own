import json
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax
import re  # re æ¨¡çµ„ä¾†é€²è¡Œæ–‡å­—çš„æ¨¡å¼æ¯”å°èˆ‡å–ä»£ï¼ˆæ¯” str.replace() æ›´å¼·å¤§ï¼‰
import os
from tqdm import tqdm



'''å¯ä¿®æ”¹åƒæ•¸'''
FOLDER_PATH = "../temp_data"  # é¸æ“‡è¦å°å“ªå€‹è³‡æ–™å¤¾åŸ·è¡Œ

OUTPUT_FOLDER_NAME = "test"  # è¨­å®šè¦å„²å­˜åˆ°çš„è³‡æ–™å¤¾åç¨±   ex. "../LCS/analysis/{OUTPUT_FOLDER_NAME}/"

JSON_DICT_NAME = "dogecoin"  # è¨­å®šæ¨æ–‡æ‰€å­˜çš„ json æª”ä¸­å­—å…¸çš„åç¨±
'''å¯ä¿®æ”¹åƒæ•¸'''




# è¼‰å…¥æ¨¡å‹
model_name = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# å®šç¾©é è™•ç†å‡½æ•¸ï¼ˆç§»é™¤@userèˆ‡httpé€£çµï¼‰
def preprocess(text):
    # ç§»é™¤æåŠï¼ˆ@usernameï¼‰  
    # \w+ï¼šä»£è¡¨ä¸€å€‹ä»¥ä¸Šçš„ã€Œè‹±æ•¸åº•ç·šçµ„æˆçš„å­—å…ƒã€ï¼Œä¹Ÿå°±æ˜¯å¸³è™Ÿæœ¬é«”    re.sub(â€¦)ï¼šè¡¨ç¤ºå°‡ç¬¦åˆé€™å€‹æ¨¡å¼çš„éƒ¨åˆ†é€šé€šç§»é™¤
    text = re.sub(r'@\w+', '', text)

    # ç§»é™¤ç¶²å€ï¼ˆhttp é–‹é ­ï¼‰
    # \S+ï¼šä¸€ä¸²ä¸æ˜¯ç©ºç™½çš„å­—å…ƒ
    text = re.sub(r'http\S+', '', text)

    # ç§»é™¤å¤šé¤˜ç©ºç™½
    text = text.strip()
    return text


if __name__ == "__main__":
    all_files = [f for f in os.listdir(FOLDER_PATH) if f.endswith(".json")]
    print(f"ğŸ“‚ ç¸½å…±æ‰¾åˆ° {len(all_files)} å€‹æª”æ¡ˆè¦è™•ç†")

    for file in all_files:
        filepath = os.path.join(FOLDER_PATH, file)
        filename = os.path.basename(filepath)  # ex: DOGE_20210428.json
        analysis_name = os.path.splitext(filename)[0]  # ex: DOGE_20210428

        # è¨­å®š txtname, json_output_path çš„åç¨±
        txtname = f"./analysis/{OUTPUT_FOLDER_NAME}/{analysis_name}.txt"
        json_output_path = f"./analysis/{OUTPUT_FOLDER_NAME}/{analysis_name}.json"

        # ç¢ºèªæ˜¯å¦æœ‰è¼¸å‡ºæ™‚éœ€ä½¿ç”¨çš„è³‡æ–™å¤¾
        output_folder_path = f"./analysis/{OUTPUT_FOLDER_NAME}/"
        os.makedirs(output_folder_path, exist_ok=True)

        # è®€å…¥ json æª”
        with open(filepath, 'r', encoding="utf-8-sig") as file:
            data_json = json.load(file)

        # æ¨æ–‡è³‡æ–™
        tweets = data_json[JSON_DICT_NAME]
        print(f"\nğŸ“„ æ­£åœ¨è™•ç†æª”æ¡ˆï¼š{filename}ï¼Œå…± {len(tweets)} ç­†æ¨æ–‡")

        # å…ˆæŠŠ txt æª”è£¡æ¸…ç©º
        with open(txtname, 'w', encoding="utf-8-sig") as filetxt:
            filetxt.write("")

        
        # å…ˆæŠŠè¼¸å‡ºçš„ json æª”è£¡æ¸…ç©º
        json_output = {JSON_DICT_NAME: []}
        with open(json_output_path, 'w', encoding='utf-8-sig') as f_json:
            json.dump(json_output, f_json, indent=4, ensure_ascii=False)

        # åˆ†ææ¯æ¢æ¨æ–‡
        labels = ['negative', 'neutral', 'positive']

        for tweet in tqdm(tweets, desc=f"ğŸ” è™•ç† {filename}"):
            # å…ˆæ¸…é™¤ @mention, http
            text = preprocess(tweet["text"])

            # æŠŠæ–‡å­—è½‰æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼ï¼ˆtoken idsã€attention maskï¼‰ï¼Œä¸¦æŒ‡å®šå›å‚³ PyTorch tensorï¼ˆ'pt'ï¼‰
            encoded_input = tokenizer(text, return_tensors='pt')

            # å¾—åˆ°è¼¸å‡ºçµæœï¼ˆlogitsï¼‰ï¼Œé€™æ˜¯ä¸€å€‹å¼µé‡ï¼ˆtensorï¼‰ï¼Œä»£è¡¨æ¯å€‹æƒ…ç·’é¡åˆ¥çš„ã€Œæœªç¶“ softmaxã€åˆ†æ•¸
            output = model(**encoded_input)

            # æŠŠæ¨¡å‹è¼¸å‡ºçš„ logits è½‰æˆæ©Ÿç‡
            # output.logitsï¼šå–å‡ºåŸå§‹çš„åˆ†æ•¸ï¼ˆtensorï¼‰
            # .detach()ï¼šåˆ‡æ–·è·Ÿ PyTorch è¨“ç·´åœ–çš„é—œè¯ï¼Œé€™æ˜¯å› ç‚ºä½ åªè¦æ¨è«–
            # .numpy()ï¼šæŠŠ tensor è½‰æˆ NumPy é™£åˆ—
            # [0]ï¼šå› ç‚ºä½ åªè¼¸å…¥ä¸€ç­†æ–‡å­—ï¼Œbatch size = 1ï¼Œæ‰€ä»¥å–å‡ºç¬¬ 0 ç­†
            # softmax(...)ï¼šå°‡ä¸‰å€‹é¡åˆ¥åˆ†æ•¸è½‰æˆæ©Ÿç‡ï¼ˆåŠ ç¸½ = 1ï¼‰
            scores = softmax(output.logits.detach().numpy()[0])

            # argmax() æœƒæ‰¾å‡ºæ©Ÿç‡æœ€å¤§å€¼çš„ç´¢å¼•
            sentiment = labels[scores.argmax()]

            with open(txtname, 'a', encoding='utf-8-sig') as txtfile:
                txtfile.write(f"Tweet {tweet['tweet_count']}\n"
                              f"Username: {tweet['username']}\n"
                              f"Text: [{repr(text)[1:-1]}]\n"
                              f"Sentiment: {sentiment}\n"
                              f"Scores: {dict(zip(labels, [round(float(s), 4) for s in scores]))}\n\n")

            # æŠŠæƒ…ç·’åˆ†æåŠ é€² json æª”ä¸­
            tweet["sentiment"] = sentiment

        
        with open(json_output_path, 'w', encoding='utf-8-sig') as f_json:
            json.dump(data_json, f_json, indent=4, ensure_ascii=False)
